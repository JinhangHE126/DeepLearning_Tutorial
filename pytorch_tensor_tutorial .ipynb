{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zstbzBSSBno"
      },
      "source": [
        "# PyTorch Tensor Basics – In-Class Tutorial\n",
        "\n",
        "This notebook is for COM6106 Week 2.\n",
        "Focus on understanding **tensor operation**.\n",
        "\n",
        "Please work through the tasks in order. For each task:\n",
        "1. Read the instructions.\n",
        "2. Fill in the `TODO` parts in the code cells.\n",
        "3. Run the cells and check the outputs.\n",
        "\n",
        "\n"
      ],
      "id": "2zstbzBSSBno"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aC25PphSBnu"
      },
      "source": [
        "## Task 1 – Tensor basics\n",
        "\n",
        "Goal: understand scalar / vector / matrix / 4D tensor and their shapes.\n",
        "Run the code and observe the outputs.\n",
        "\n"
      ],
      "id": "3aC25PphSBnu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuFY2gRKSBnu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Four types of tensors\n",
        "# TODO 1: create a scalar tensor 3.14\n",
        "scalar = None\n",
        "\n",
        "# TODO 2: create a vector tensor [1.0, 2.0, 3.0]\n",
        "vector = None\n",
        "\n",
        "# TODO 3: create a matrix with shape 2*3 and all elements are 1\n",
        "matrix = None\n",
        "\n",
        "# TODO 4: create a rank 4 tensor with shape (4, 3, 28, 28) and all elements are 0\n",
        "tensor4d = None\n",
        "\n",
        "for name, x in [(\"scalar\", scalar),\n",
        "                (\"vector\", vector),\n",
        "                (\"matrix\", matrix),\n",
        "                (\"tensor4d\", tensor4d)]:\n",
        "    print(f\"{name}: shape = {x.shape}, dim = {x.dim()}, numel = {x.numel()}\")\n"
      ],
      "id": "NuFY2gRKSBnu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRb3TT4MSBnu"
      },
      "source": [
        "## Task 2.1 – 1D tensor slicing\n",
        "\n",
        "We have a 1D tensor:\n",
        "\n",
        "```python\n",
        "x = torch.arange(10)   # [0,1,2,3,4,5,6,7,8,9]\n",
        "```\n",
        "\n",
        "Please complete the TODOs using slicing.\n",
        "\n",
        "**Goals:**\n",
        "- Practice `x[start:end]` and `x[start:end:step]`.  \n"
      ],
      "id": "xRb3TT4MSBnu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX06azQRSBnu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.arange(10)   # [0,1,2,3,4,5,6,7,8,9]\n",
        "print(\"x:\", x)\n",
        "\n",
        "# TODO 1: select elements 2,3,4,5\n",
        "part1 = None\n",
        "print(\"part1 (should be [2,3,4,5]):\", part1)\n",
        "\n",
        "# TODO 2: select elements at even indices (0,2,4,6,8)\n",
        "even_index = None\n",
        "print(\"even_index (should be [0,2,4,6,8]):\", even_index)\n",
        "\n",
        "# TODO 3: select only the odd values `1,3,5,7,9`\n",
        "odd_values = None\n",
        "print(\"odd_values:\", odd_values)\n"
      ],
      "id": "CX06azQRSBnu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xa6nrMLSBnv"
      },
      "source": [
        "## Task 2.2 – 2D tensor slicing\n",
        "\n",
        "We create a 3x4 matrix:\n",
        "\n",
        "```python\n",
        "M = torch.arange(1, 13).view(3, 4)\n",
        "```\n",
        "\n",
        "**TODOs:**\n",
        "1. Select the 2nd row.\n",
        "2. Select the 1st column.\n",
        "3. Select the top-right 2x2 block (first two rows, last two columns).\n",
        "4. Reverse the last row.\n",
        "5. Select the diagonal elements.\n"
      ],
      "id": "9Xa6nrMLSBnv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnMVwNmhSBnv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "M = torch.arange(1, 13).view(3, 4)  # 3x4 matrix: numbers 1~12\n",
        "print(\"M =\\n\", M)\n",
        "\n",
        "# TODO 1: select the 2nd row\n",
        "row2 = None\n",
        "print(\"row2:\", row2)\n",
        "\n",
        "# TODO 2: select the 1st column\n",
        "col1 = None\n",
        "print(\"col1:\", col1, \"shape:\", None if col1 is None else col1.shape)\n",
        "\n",
        "# TODO 3: select the top-right 2x2 block\n",
        "block = None\n",
        "print(\"block:\\n\", block)\n",
        "\n",
        "# TODO 4: select the diagonal elements.\n",
        "diagonal = None\n",
        "print(\"diagonal elements:\", diagonal)"
      ],
      "id": "vnMVwNmhSBnv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBXqAQjJSBnv"
      },
      "source": [
        "## Task 2.3 – 4D \"image batch\" slicing\n",
        "\n",
        "Now we simulate a batch of images:\n",
        "\n",
        "```python\n",
        "images = torch.randn(2, 3, 28, 28)  # [Batch, Channel, Height, Width]\n",
        "```\n",
        "\n",
        "**TODOs:**\n",
        "1. Take the 1st image (shape: `[3, 28, 28]`).\n",
        "2. Take the 2nd channel of the 1st image (shape: `[28, 28]`).\n",
        "3. From the 2nd image, take the top-left 5x5 patch of all channels (shape: `[3, 5, 5]`).\n",
        "4. Take all images but only the 1st channel (shape: `[2, 28, 28]`).\n",
        "\n"
      ],
      "id": "PBXqAQjJSBnv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI1X82hKSBnw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "images = torch.randn(2, 3, 28, 28)   # [Batch, Channel, Height, Width]\n",
        "print(\"images shape:\", images.shape)\n",
        "\n",
        "# TODO 1: first image\n",
        "img1 = None\n",
        "print(\"img1 shape (should be [3,28,28]):\", None if img1 is None else img1.shape)\n",
        "\n",
        "# TODO 2: 2nd channel of the 1st image, shape [28,28]\n",
        "img1_ch2 = None\n",
        "print(\"img1_ch2 shape (should be [28,28]):\", None if img1_ch2 is None else img1_ch2.shape)\n",
        "\n",
        "# TODO 3: top-left 5x5 patch of 2nd image, all channels -> shape [3,5,5]\n",
        "patch = None\n",
        "print(\"patch shape (should be [3,5,5]):\", None if patch is None else patch.shape)\n",
        "\n",
        "# TODO 4: all images, only 1st channel -> shape [2,28,28]\n",
        "one_channel = None\n",
        "print(\"one_channel shape (should be [2,28,28]):\", None if one_channel is None else one_channel.shape)\n"
      ],
      "id": "lI1X82hKSBnw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 Matrix Multiple\n",
        "\n",
        "Recall the in pytorch, matrix multiple is using torch.matmal.\n",
        "\n",
        "In this task, we will implement them without using calling torch.matmul (or use @), but implement them by yourselves.\n",
        "\n",
        "We ask you to implement it in two ways."
      ],
      "metadata": {
        "id": "7D9nYwtfSh3P"
      },
      "id": "7D9nYwtfSh3P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, implement it using two-nested forloop (exactly two nested forloop).\n",
        "\n",
        "Hint: you need torch.sum."
      ],
      "metadata": {
        "id": "tJ1liN8FSsXJ"
      },
      "id": "tJ1liN8FSsXJ"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "import scipy.interpolate"
      ],
      "metadata": {
        "id": "jmY6lXeDT8mf"
      },
      "id": "jmY6lXeDT8mf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utilities\n",
        "\n",
        "def is_same_tensor(result: torch.Tensor,\n",
        "                   ref: torch.Tensor,\n",
        "                   tol: Optional[float]=None) -> bool:\n",
        "  \"\"\"\n",
        "  Check if two tensors are the same.\n",
        "\n",
        "  Args:\n",
        "    result: Results by your code.\n",
        "    ref: Ground truth result.\n",
        "\n",
        "  Return:\n",
        "    Whether result and ref are the same.\n",
        "  \"\"\"\n",
        "  if (not isinstance(result, torch.Tensor) or\n",
        "      not isinstance(ref, torch.Tensor)):\n",
        "    return False\n",
        "  if result.dtype != ref.dtype:\n",
        "    result = result.to(ref.dtype)\n",
        "  if tol is not None:\n",
        "    return torch.allclose(result, ref, rtol=0, atol=tol)\n",
        "  else:\n",
        "    return torch.equal(result, ref)"
      ],
      "metadata": {
        "id": "nP0U-S-3T-dh"
      },
      "id": "nP0U-S-3T-dh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Task 3.1 matmul_forloop\n",
        "\n",
        "def matmul_forloop(\n",
        "    x: torch.Tensor,\n",
        "    y: torch.Tensor\n",
        ") -> torch.tensor:\n",
        "  \"\"\"\n",
        "  Using python forloop to implement torch.matmul.\n",
        "\n",
        "  Args:\n",
        "    x: First matrix.\n",
        "    y: Second matrix.\n",
        "\n",
        "  Returns:\n",
        "    Result matrix. If two input do not match, return None.\n",
        "  \"\"\"\n",
        "\n",
        "  #### Your code goes here\n",
        "  return None"
      ],
      "metadata": {
        "id": "S9zfxvCqSue1"
      },
      "id": "S9zfxvCqSue1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, implement it using vector/matrix operations. No forloop statement is allowed this time.\n",
        "\n",
        "Hints: use torch.sum and broadcast."
      ],
      "metadata": {
        "id": "4izv7S2jSw82"
      },
      "id": "4izv7S2jSw82"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Task 3.2 matmul_nofor\n",
        "\n",
        "def matmul_nofor(\n",
        "    x: torch.tensor,\n",
        "    y: torch.tensor\n",
        ") -> torch.tensor:\n",
        "  \"\"\"\n",
        "  Using pytorch vector operations to implement torch.matmul. No forloop is\n",
        "  allowed.\n",
        "\n",
        "  Args:\n",
        "    x: First matrix.\n",
        "    y: Second matrix.\n",
        "\n",
        "  Returns:\n",
        "    Result matrix. If two input do not match, return None.\n",
        "  \"\"\"\n",
        "\n",
        "  #### Your code goes here\n",
        "  return None"
      ],
      "metadata": {
        "id": "OzZvthTXSvX3"
      },
      "id": "OzZvthTXSvX3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, implement it using einsum."
      ],
      "metadata": {
        "id": "yoxdVjV4S3m4"
      },
      "id": "yoxdVjV4S3m4"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Task 3.3 matmul_einsum\n",
        "\n",
        "def matmul_einsum(\n",
        "    x: torch.tensor,\n",
        "    y: torch.tensor\n",
        ") -> torch.tensor:\n",
        "  \"\"\"\n",
        "  Using pytorch vector operations to implement torch.matmul. No forloop is\n",
        "  allowed.\n",
        "\n",
        "  Args:\n",
        "    x: First matrix.\n",
        "    y: Second matrix.\n",
        "\n",
        "  Returns:\n",
        "    Result matrix. If two input do not match, return None.\n",
        "  \"\"\"\n",
        "\n",
        "  #### Your code goes here\n",
        "  return None"
      ],
      "metadata": {
        "id": "SIHSgRkUS6ez"
      },
      "id": "SIHSgRkUS6ez",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing\n",
        "\n",
        "**Run this block to check if the results are correct**.  "
      ],
      "metadata": {
        "id": "5xgcQs-vTsxc"
      },
      "id": "5xgcQs-vTsxc"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test 1\n",
        "\n",
        "dim1list = [2, 3, 10, 30]\n",
        "dim2list = [2, 1, 5, 100]\n",
        "dim3list = [2, 2, 10, 100]\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "for i, (dim1, dim2, dim3) in enumerate(zip(dim1list, dim2list, dim3list)):\n",
        "  a = torch.randint(0, 100, size=(dim1, dim2))\n",
        "  b = torch.randint(0, 100, size=(dim2, dim3))\n",
        "  c_ref = a @ b\n",
        "  c_forloop = matmul_forloop(a, b)\n",
        "  c_nofor = matmul_nofor(a, b)\n",
        "  c_einsum = matmul_einsum(a, b)\n",
        "  assert is_same_tensor(c_ref, c_forloop)\n",
        "  assert is_same_tensor(c_ref, c_nofor)\n",
        "  assert is_same_tensor(c_ref, c_einsum)\n",
        "  print(f'{i}-th test succeeds')"
      ],
      "metadata": {
        "id": "q1e4XbCXT0WG"
      },
      "id": "q1e4XbCXT0WG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test 2\n",
        "\n",
        "dim1 = 500\n",
        "dim2 = 1000\n",
        "dim3 = 2000\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "a = torch.rand(size=(dim1, dim2), dtype=torch.float32)\n",
        "b = torch.rand(size=(dim2, dim3), dtype=torch.float32)\n",
        "c_ref = a @ b\n",
        "c_nofor = matmul_nofor(a, b)\n",
        "c_einsum = matmul_einsum(a, b)\n",
        "assert is_same_tensor(c_ref, c_nofor, 1e-3)\n",
        "assert is_same_tensor(c_ref, c_einsum, 1e-3)\n",
        "print('test succeeds')"
      ],
      "metadata": {
        "id": "QbeaUavCT2UM"
      },
      "id": "QbeaUavCT2UM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test 3\n",
        "\n",
        "a = torch.rand(size=(3, 5))\n",
        "b = torch.rand(size=(3, 5, 7))\n",
        "assert matmul_forloop(a, b) == None\n",
        "assert matmul_nofor(a, b) == None\n",
        "assert matmul_einsum(a, b) == None\n",
        "\n",
        "a = torch.rand(size=(3, 5))\n",
        "b = torch.rand(size=(3, 5))\n",
        "assert matmul_forloop(a, b) == None\n",
        "assert matmul_nofor(a, b) == None\n",
        "assert matmul_einsum(a, b) == None\n",
        "\n",
        "a = torch.rand(size=(3, 5))\n",
        "b = torch.rand(size=(7, 4))\n",
        "assert matmul_forloop(a, b) == None\n",
        "assert matmul_nofor(a, b) == None\n",
        "assert matmul_einsum(a, b) == None\n",
        "\n",
        "print('test succeeds')"
      ],
      "metadata": {
        "id": "JNLiDo1WT3uQ"
      },
      "id": "JNLiDo1WT3uQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Other Operations**"
      ],
      "metadata": {
        "id": "EDxrGezmS9Bw"
      },
      "id": "EDxrGezmS9Bw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4 – `flip`\n",
        "\n",
        "We create a 3D tensor of shape `[2, 2, 2]` with integer dtype.\n",
        "\n",
        "So M looks like 2 small 2×2 blocks stacked together:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gSo0IcSReVhJ"
      },
      "id": "gSo0IcSReVhJ"
    },
    {
      "cell_type": "code",
      "source": [
        "M = torch.arange(8, dtype=torch.int32).view(2, 2, 2)\n",
        "print('M:', M)"
      ],
      "metadata": {
        "id": "X4CotmV0edKz"
      },
      "id": "X4CotmV0edKz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODOs:**\n",
        "\n",
        "flip the two blocks and also flip each block by rows.\n",
        "\n",
        "hint: the 'block' dim is 0 and row dim is 1"
      ],
      "metadata": {
        "id": "9ppXBySxejBc"
      },
      "id": "9ppXBySxejBc"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:\n",
        "M_flip = None\n",
        "print(\"M_flip = \", M_flip)"
      ],
      "metadata": {
        "id": "e0m4h-U_eiUb"
      },
      "id": "e0m4h-U_eiUb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsgp-IwdSBnw"
      },
      "source": [
        "## Task 5 – `squeeze` and `unsqueeze`\n",
        "\n",
        "`squeeze` removes dimensions of size 1.  \n",
        "`unsqueeze` adds a new dimension of size 1.\n",
        "\n",
        "These operations are very common when working with images and batches.\n",
        "\n",
        "**TODOs:**\n",
        "1. Use `squeeze` to remove the channel dimension (size 1) and get shape `[4, 28, 28]`.\n",
        "2. Use `unsqueeze` to add the channel dimension back and get shape `[4, 1, 28, 28]`.\n",
        "3. For a 1D vector `v`, use `unsqueeze` to make it a row vector `[1, 3]`.\n",
        "4. For the same `v`, use `unsqueeze` to make it a column vector `[3, 1]`.\n"
      ],
      "id": "rsgp-IwdSBnw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lX8XyX5SBnw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Example 1: image tensor with a channel dimension of size 1\n",
        "x = torch.rand(4, 1, 28, 28)\n",
        "print(\"original x shape:\", x.shape)\n",
        "\n",
        "# TODO 1: use squeeze to remove the channel dimension (dim=1)\n",
        "x_squeezed = None\n",
        "print(\"x_squeezed shape:\", None if x_squeezed is None else x_squeezed.shape)\n",
        "\n",
        "# TODO 2: use unsqueeze to add the channel dimension back at dim 1\n",
        "x_unsqueezed = None\n",
        "print(\"x_unsqueezed shape:\", None if x_unsqueezed is None else x_unsqueezed.shape)\n",
        "\n",
        "# Example 2: a simple vector\n",
        "v = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(\"v shape:\", v.shape)\n",
        "\n",
        "# TODO 3: use unsqueeze to get shape [1,3]\n",
        "v_row = None\n",
        "print(\"v_row shape (should be [1,3]):\", None if v_row is None else v_row.shape)\n",
        "\n",
        "# TODO 4: use unsqueeze to get shape [3,1]\n",
        "v_col = None\n",
        "print(\"v_col shape (should be [3,1]):\", None if v_col is None else v_col.shape)\n"
      ],
      "id": "1lX8XyX5SBnw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8C85MISSBnw"
      },
      "source": [
        "## Task 6 – `view` / `reshape` and number of elements\n",
        "\n",
        "We have a batch of 4 images with shape `[4, 1, 28, 28]`.\n",
        "\n",
        "**TODOs:**\n",
        "1. Flatten each image to a 784-dimensional vector to get shape `[4, 784]`.\n",
        "2. Reshape it back to `[4, 1, 28, 28]`.\n",
        "3. Run the \"bad view\" example and read the error message.\n"
      ],
      "id": "j8C85MISSBnw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7zBgYIoSBnw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.rand(4, 1, 28, 28)\n",
        "print(\"original shape:\", x.shape, \"numel =\", x.numel())\n",
        "\n",
        "# TODO 1: flatten each image -> [4, 784]\n",
        "x_flat = None\n",
        "print(\"x_flat shape (should be [4,784]):\", None if x_flat is None else x_flat.shape)\n",
        "\n",
        "# TODO 2: reshape back to [4, 1, 28, 28]\n",
        "x_back = None\n",
        "print(\"x_back shape (should be [4,1,28,28]):\", None if x_back is None else x_back.shape)\n",
        "\n",
        "# bad view example\n",
        "try:\n",
        "    bad = x.view(4, 783)   # this should fail\n",
        "except RuntimeError as e:\n",
        "    print(\"RuntimeError from bad view:\", e)\n"
      ],
      "id": "I7zBgYIoSBnw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xqq7VZaSBnw"
      },
      "source": [
        "## Task 7 – `transpose` vs `permute`\n",
        "\n",
        "We create a 4D tensor `b` with shape `[B, C, H, W] = [4, 3, 28, 32]`.\n",
        "\n",
        "**TODOs:**\n",
        "1. Use `transpose` to swap the height and width dimensions (2nd and 3rd spatial dims).\n",
        "2. Use `permute` to reorder the tensor to shape `[B, H, W, C]`.\n"
      ],
      "id": "8Xqq7VZaSBnw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWbx_2CJSBnw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "b = torch.rand(4, 3, 28, 32)  # [B, C, H, W]\n",
        "print(\"b shape:\", b.shape)\n",
        "\n",
        "# TODO 1: swap H and W using transpose\n",
        "b_t = None\n",
        "print(\"b_t shape:\", None if b_t is None else b_t.shape)\n",
        "\n",
        "# TODO 2: reorder to [B, H, W, C] using permute\n",
        "b_p = None\n",
        "print(\"b_p shape:\", None if b_p is None else b_p.shape)\n"
      ],
      "id": "mWbx_2CJSBnw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJRLuk6ZSBnw"
      },
      "source": [
        "## Task 8 – `cat` vs `stack`\n",
        "\n",
        "We have two batches of images.\n",
        "\n",
        "**TODOs:**\n",
        "1. Use `torch.cat` to concatenate `a1` and `a2` along the batch dimension (dim=0), getting shape `[9, 3, 32, 32]`.\n",
        "2. Use `torch.stack` to stack `b1` and `b2` along a new dimension, getting shape `[2, 4, 3, 32, 32]`.\n"
      ],
      "id": "hJRLuk6ZSBnw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9POAmWSSBnw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "a1 = torch.rand(4, 3, 32, 32)   # batch 1\n",
        "a2 = torch.rand(5, 3, 32, 32)   # batch 2\n",
        "\n",
        "# TODO 1: concatenate along the first dimension -> [9,3,32,32]\n",
        "cat_batch = None\n",
        "print(\"cat_batch shape:\", None if cat_batch is None else cat_batch.shape)\n",
        "\n",
        "# Same batch size for stacking\n",
        "b1 = torch.rand(4, 3, 32, 32)\n",
        "b2 = torch.rand(4, 3, 32, 32)\n",
        "\n",
        "# TODO 2: stack along a new dimension -> [2,4,3,32,32]\n",
        "stack_batch = None\n",
        "print(\"stack_batch shape:\", None if stack_batch is None else stack_batch.shape)\n"
      ],
      "id": "c9POAmWSSBnw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz9iEXsKSBnx"
      },
      "source": [
        "## Task 9 – CPU vs GPU speed\n",
        "\n",
        "Use GPU provided by Colab, run the following code to compare matrix multiplication speed.\n"
      ],
      "id": "Iz9iEXsKSBnx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment check\n",
        "\n",
        "Goal: make sure PyTorch can be imported and the runtime is working.\n",
        "\n"
      ],
      "metadata": {
        "id": "FvOge68jjfRj"
      },
      "id": "FvOge68jjfRj"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# TODO: Use GPU as runtime\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "KWqIJ0awjgOy"
      },
      "id": "KWqIJ0awjgOy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMDd9DZaSBnx"
      },
      "outputs": [],
      "source": [
        "import torch, time\n",
        "\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "device_gpu = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "a_cpu = torch.rand(2000, 2000, device=device_cpu)\n",
        "b_cpu = torch.rand(2000, 2000, device=device_cpu)\n",
        "\n",
        "t0 = time.time()\n",
        "c_cpu = a_cpu @ b_cpu\n",
        "t1 = time.time()\n",
        "print(\"CPU matmul time:\", t1 - t0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    a_gpu = a_cpu.to(device_gpu)\n",
        "    b_gpu = b_cpu.to(device_gpu)\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    c_gpu = a_gpu @ b_gpu\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    print(\"GPU matmul time:\", t1 - t0)\n",
        "else:\n",
        "    print(\"No GPU available.\")\n"
      ],
      "id": "CMDd9DZaSBnx"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}